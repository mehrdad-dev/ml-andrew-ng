<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته هشتم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/</link>
    <description>Recent content in  هفته هشتم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Oct 2020 12:16:41 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week8/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>مقدمه یادگیری بدون نظارت</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/unsupervised-learning-introduction/</link>
      <pubDate>Mon, 19 Oct 2020 12:22:06 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/unsupervised-learning-introduction/</guid>
      <description>یادگیری بدون نظارت در تضاد با یادگیری با نظارت است، زیرا از یک مجموعه آموزشی بدون لیبل استفاده می‌کند.
به عبارت دیگر، ما بردار $y$ را به عنوان نتایج مورد انتظار نداریم، فقط مجموعه داده ای از ویژگی ها داریم که می‌خواهیم ساختاری در آن ها پیدا کنیم.
طبقه بندی برای موراد زیر خوب است:
 تقسیم بندی بازار تحلیل شبکه های اجتماعی سازماندهی خوشه های رایانه ای تجزیه و تحلیل داده های نجومی  </description>
    </item>
    
    <item>
      <title>الگوریتم K-Means</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/k-means/</link>
      <pubDate>Mon, 19 Oct 2020 12:43:35 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/k-means/</guid>
      <description>الگوریتم K-Means محبوب ترین و پرکاربردترین الگوریتم برای گروه بندی خودکار داده ها در زیر مجموعه های منسجم (اعضای آن به هم مربوط هستند) است.
 به طور تصادفی دو نقطه از مجموعه داده ها را به نام مرکز خوشه ای مقدار دهی می‌کنیم تخصیص خوشه: همه مثالها را به یکی از دو گروه تقسیم کنید بر اساس اینکه به کدام مرکز خوشه نزدیک است میانگین تمام نقاط داخل هر دو گروه مرکز خوشه ای را محاسبه کنید، سپس نقاط مرکز خوشه را به آن میانگین ها منتقل کنید مرحله ۲ و ۳ را دوباره اجرا کنید تا زمانی که خوشه های خود را پیدا کنید  متغیرهای اصلی ما عبارتند از:</description>
    </item>
    
    <item>
      <title>بهینه سازی هدفمند</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/optimization-objective/</link>
      <pubDate>Thu, 22 Oct 2020 17:08:35 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/optimization-objective/</guid>
      <description>برخی از پارامتر هایی را که در الگوریتم خود استفاده کردیم به یاد بیاورید:
 $ = c ^ {(i)}$ ایندکس خوشه ای ${1,2,&amp;hellip;,k}$ که به نمونه $x^{(i)}$ منتسب شده است. $ = \mu _k $ خوشه مرکزی k $(\mu _k \in \mathbb{R} ^ n)$ $ = \mu _ {c ^ {(i)}} $ مرکز خوشه ای که به نمونه $x^{(i)}$ منتسب شد است.  با استفاده از این متغیرها می‌توانیم تابع هزینه خود را تعریف کنیم:</description>
    </item>
    
    <item>
      <title>مقداردهی اولیه تصادفی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/random-initialization/</link>
      <pubDate>Sat, 24 Oct 2020 12:35:28 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/random-initialization/</guid>
      <description>یک روش پیشنهادی برای مقداردهی اولیه تصادفی برای مرکز های خوشه ای وجود دارد.
  اگر داشته باشید $k &amp;lt; m$، یعنی اطمینان حاصل کنید که تعداد خوشه های شما از تعداد نمونه های آموزشی شما کمتر است.
  به طور تصادفی نمونه های آموزشی $k$ را انتخاب کنید.(مطمعن شوید که نمونه ها منحصر به فرد باشند)
  $\mu_1, &amp;hellip;, \mu _k$ ها را برابر با نمونه های $k$ قرار بدهید.</description>
    </item>
    
    <item>
      <title>انتخاب تعداد خوشه ها</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/choosing-the-number-of-clusters/</link>
      <pubDate>Sat, 24 Oct 2020 13:13:58 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/choosing-the-number-of-clusters/</guid>
      <description>انتخاب $K$ می‌تواند کاملا خودسرانه و مبهم باشد.
روش elbow: نمودار هزینه $J$ و تعداد خوشه های $K$ را رسم کنید، با افزایش تعداد خوشه ها مقدار تابع هزینه باید کاهش یابد، و سپس یکسان شود. $K$ را در نقطه ای انتخاب کنید که تابع هزینه در حال یکسان شدن است.
با افزایش $K$، $J$ همیشه کاهش می‌یابد. اما یک استثنا وجود دارد اگر k-means در مینیمم محلی گیر کند.</description>
    </item>
    
    <item>
      <title>کاهش ابعاد</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/dimensionality-reduction/</link>
      <pubDate>Sun, 25 Oct 2020 12:57:20 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/dimensionality-reduction/</guid>
      <description>انگیزه ۱: فشرده سازی داده ها  اگر داده های زائد زیادی داشته باشیم ممکن است بخواهیم ابعاد ویژگی های خود را کاهش دهیم. برای انجام این کار، دو ویژگی بسیار مرتبط پیدا می‌کنیم، آنها را رسم می‌کنیم و یک خط جدید ایجاد می‌کنیم که به نظر می‌رسد هر دو ویژگی را به طور دقیق توصیف می‌کند. ما همه ویژگی های جدید را روی این خط واحد قرار می‌دهیم.  انجام کاهش ابعاد باعث کاهش کل داده هایی می‌شود که باید در حافظه کامپیوتر ذخیره کنیم و الگوریتم یادگیری ما را تسریع می‌کند.</description>
    </item>
    
    <item>
      <title>تحلیل اجزای اصلی فرمول مسئله</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/pca-problem-formulation/</link>
      <pubDate>Sun, 25 Oct 2020 13:17:54 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/pca-problem-formulation/</guid>
      <description>محبوب ترین الگوریتم کاهش ابعاد، تحلیل اجزای اصلی یا PCA است.
فرمول مسئله باتوجه به ۲ ویژگی $x_1$ و $x_2$ ،ما می‌خواهیم یک خط واحد پیدا کنیم که به طور موثر هر دو ویژگی را همزمان توصیف کند. سپس ویژگی های قدیمی خود را بر روی این خط جدید ترسیم می‌کنیم تا یک ویژگی واحد جدید بدست آوریم.
همین کار را می توان با سه ویژگی انجام داد.
هدف PCA کاهش میانگین تمام فواصل هر ویژگی تا خط projection است.</description>
    </item>
    
    <item>
      <title>تحلیل اجزای اصلی الگوریتم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/pca-algorithm/</link>
      <pubDate>Mon, 26 Oct 2020 09:17:16 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/pca-algorithm/</guid>
      <description>قبل از استفاده از PCA ، یک مرحله پیش پردازش داده وجود دارد که باید انجام دهیم:
پیش پردازش داده   گرفتن مجموعه آموزشی: $x^{(1)}, x^{(2)}, &amp;hellip; , x^{(m)}$
  پیش پردازش (feature scaling/mean normalization): $\mu _j = \frac{1}{m} \sum _ {i=1} ^ m x ^ {(i)} _j $
  جایگزین کردن هر $x ^ {(i)} _j $ با $x ^ {(i)} _j - \mu _j$
  اگر ویژگی های مختلف در مقیاس های متفاوت باشند (مثلا $= x_1$ اندازه خانه، $ = x_2$ تعداد اتاق خواب ها)، ویژگی ها را برای داشتن دامنه ای قابل مقایسه از مقادیر تغییر مقیاس دهید.</description>
    </item>
    
    <item>
      <title>بازسازی از نمایش فشرده</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week8/reconstruction-compressed-representation/</link>
      <pubDate>Thu, 12 Nov 2020 12:02:43 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week8/reconstruction-compressed-representation/</guid>
      <description>اگر برای فشرده سازی داده های خود از PCA استفاده می‌کنیم، چگونه می‌توانیم داده های خود را از حالت فشرده خارج کنیم یا به تعداد اصلی ویژگی های خود برگردیم؟
برای بازگشت از حالت 1 بعدی به 2 بعدی، این کار را انجام می‌دهیم: $z \in \mathbb{R} \rightarrow x \in \mathbb{R ^ 2} $
ما می‌توانیم این کار را با یک معادله انجام دهیم: $ x ^{(i)} _ {approx} = Ureduce \cdot z ^{(i)} $</description>
    </item>
    
  </channel>
</rss>