<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته پنجم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/</link>
    <description>Recent content in  هفته پنجم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Sep 2020 17:44:01 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>تابع هزینه شبکه عصبی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</link>
      <pubDate>Wed, 30 Sep 2020 17:52:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</guid>
      <description>اجازه دهید برای شروع چند متغیر تعریف کنیم:
   متغیر      $L$ تعداد کل لایه ها در شبکه عصبی   $s_l$ تعداد نود ها در لایه $l$ ام (بدون احتساب نود بایاس)   $K$ تعداد کلاس های خروجی    به یاد بیاورید که در شبکه های عصبی ممکن است تعداد نود های خروجی زیادی داشته باشیم. ما $h_\Theta(x)_k$ را به عنوان یک فرضیه در نظر می‌گیریم، که منجر به خروجی $k^{th}$ می‌شود.</description>
    </item>
    
    <item>
      <title>Backpropagation قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</link>
      <pubDate>Thu, 01 Oct 2020 12:30:31 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</guid>
      <description>اصطلاح Backpropagation یا به فارسی پس انتشار در شبکه های عصبی، برای به حداقل رساندن تابع هزینه (minimizing cost function) مثل کاری که با گرادیان کاهشی در رگرسیون لجستیک و خطی انجام می‌دادیم، استفاده می‌شود.
هدف ما محاسبه این است:
$$ min_\Theta J(\Theta) $$
یعنی می‌خواهیم تابع هزیه $J$ را با استفاده از یک محموعه بهینه از پارامتر $\Theta$ به حداقل برسانیم (یا به عبارتی دیگر مینیمم کنیم).
در این بخش به معادلاتی که برای محاسبه مشتق جزئی تابع $J(\Theta)$ استفاده می‌کنیم، خواهیم پرداخت:</description>
    </item>
    
    <item>
      <title>Backpropagation قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</link>
      <pubDate>Thu, 08 Oct 2020 12:25:51 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</guid>
      <description>به یاد بیاورید که تابع هزینه برای یک شبکه عصبی به این صورت بود:
$$ \begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*} $$
اگر یک طبقه ساده، و غیر چند کلاسه (non-multiclass, $k=1$) را در نظر بگیریم و همچنین منظم سازی یا همان regularization را هم نادیده بگیریم، هزینه ما به این صورت محاسبه خواهد شد:</description>
    </item>
    
    <item>
      <title>Unrolling Parameters</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/unrolling-parameters/</link>
      <pubDate>Sat, 10 Oct 2020 13:31:21 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/unrolling-parameters/</guid>
      <description>ما در شبکه های عصبی در حال کار با مجموعه ای از ماتریس ها هستیم: $$ \begin{align*} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align*} $$
برای استفاده از تابع بهینه سازی مثل ()fminunc، می‌خواهیم همه عضو ها را unroll کرده (باز کنیم) و داخل یک وکتور طولانی قرار دهیم:
thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ]  اگر ابعاد Theta1 $10 \times 11$ و Theta2 $10 \times 11$ و Theta3 $1 \times 11$ باشند، سپس می‌توانیم ماتریس های اصلی مان را از نسخه های unrolled به این شکل برگردانیم:</description>
    </item>
    
    <item>
      <title>بررسی گرادیان</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/gradient-checking/</link>
      <pubDate>Sun, 11 Oct 2020 16:33:27 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/gradient-checking/</guid>
      <description>بررسی گرادیان یا همان Gradient checking به شما این اطمینان را می‌دهد که Backpropagation همانطور که در نظر گرفته شده کار می‌کند‌ (مطابق هدف کار می‌کند).
ما می‌توانیم مشتق تابع خود را به این صورت تقریب بزنیم: $$ \frac{\partial}{\partial \Theta} J(\Theta) \approx \frac { J(\Theta + \epsilon) - J(\Theta - \epsilon) } {2 \epsilon} $$
با چند ماتریس تتا، می‌توانیم مشتق را با توجه به $\Theta _j$ تقریب بزنیم:
$$ \frac{\partial}{\partial \Theta} J(\Theta) \approx \frac{ J(\Theta _1, &amp;hellip;, \Theta _j + \epsilon, &amp;hellip;, \Theta _n) - J(\Theta _1, &amp;hellip;, \Theta _j - \epsilon, &amp;hellip;, \Theta _n)} {2 \epsilon} $$</description>
    </item>
    
    <item>
      <title>مقدار دهی اولیه تصادفی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/random-initialization/</link>
      <pubDate>Sun, 11 Oct 2020 17:13:59 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/random-initialization/</guid>
      <description>مقدار دهی اولیه همه وزن های تتا (theta weights) به مقدار $0$ برای شبکه های عصبی کار ساز نیست!
وقتی از Backpropagation استفاده می‌کنیم، همه گره ها به طور مکرر به یک مقدار مشابه به روز می‌شوند. اما در عوض می‌توانیم وزن های ماتریس $\Theta$ خودمان را به روش زیر به صورت تصادفی مقدار دهی کنیم:
از این رو، ما هر $\Theta _{ij} ^{(l)}$ را به صورت عددی تصادفی بین $[ - \epsilon, \epsilon]$ مقدار دهی می‌کنیم، و استفاده از فرمول بالا تضمین می‌کند که این حد مد نظر را به دست می‌آوریم، همین رویه برای همه $\Theta$ ها انجام می‌شود.</description>
    </item>
    
    <item>
      <title>Putting It Together</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/putting-it-together/</link>
      <pubDate>Sun, 11 Oct 2020 19:45:27 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/putting-it-together/</guid>
      <description>ابتدا معماری شبکه خود را انتخاب کنید!
لایه های شبکه عصبی خود را انتخاب کنید، از جمله اینکه چند گره پنهان در هر لایه و در کل چند لایه می‌خواهد داشته باشید.
 تعداد گره های ورودی = ابعاد ویژگی های $x^{(i)}$ تعداد گره های خروجی = تعداد کلاس ها (طبقه بندی ها) تعداد گره های پنهان در هر لایه = معمولا هر چه بیشتر بهتر (افزایش تعداد گره های پنهان باید با هزینه محاسبه آن ها تعادل داشته باشد) پیشفرض ها: ۱ لایه پنهان، اگر بیش از ۱ لایه پنهان دارید پیشنهاد می‌شود که در هر لایه پنهان تعداد گره یکسانی داشته باشید.</description>
    </item>
    
    <item>
      <title>نمونه ای از رانندگی خودکار</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/autonomous-driving/</link>
      <pubDate>Mon, 12 Oct 2020 13:09:44 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/autonomous-driving/</guid>
      <description>پروژه ALVINN ماشین خودران در سال ۱۹۸۹ ALVINN: Autonomous Land Vehicle In a Neural Network
برای مشاهده ویدئو این پروژه در YouTube بر روی تصویر زیر کلیک کنید:

توییت ای جالب در توییتر در این مورد (نیاز به vpn):
GPU? Gez, ALVINN ran on 100 MFLOP CPU, ~10x slower than iWatch; Refrigerator-size &amp;amp; needed 5000 watt generator. @olivercameron pic.twitter.com/QdGpZUzGCs
&amp;mdash; Dean Pomerleau (@deanpomerleau) November 24, 2016  </description>
    </item>
    
    <item>
      <title>فایل های هفته پنجم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/files/</link>
      <pubDate>Mon, 12 Oct 2020 13:33:56 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/files/</guid>
      <description>اسلاید ها  Neural Networks: Learning - pdf  غلط نامه  Errata - pdf  تمرین برنامه نویسی  Programming Exercise 4: Neural Networks Learning - pdf | problem  </description>
    </item>
    
  </channel>
</rss>