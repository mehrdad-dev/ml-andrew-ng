<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته پنجم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/</link>
    <description>Recent content in  هفته پنجم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Sep 2020 17:44:01 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>تابع هزینه شبکه عصبی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</link>
      <pubDate>Wed, 30 Sep 2020 17:52:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</guid>
      <description>اجازه دهید برای شروع چند متغیر تعریف کنیم:
   متغیر      $L$ تعداد کل لایه ها در شبکه عصبی   $s_l$ تعداد گره ها در لایه $l$ ام (بدون احتساب گره بایاس)   $K$ تعداد کلاس های خروجی    به یاد بیاورید که در شبکه های عصبی ممکن است تعداد گره های خروجی زیادی داشته باشیم. ما $h_\Theta(x)_k$ را به عنوان یک فرضیه در نظر می‌گیریم، که منجر به خروجی $k^{th}$ می‌شود.</description>
    </item>
    
    <item>
      <title>پس انتشار قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</link>
      <pubDate>Thu, 01 Oct 2020 12:30:31 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</guid>
      <description>پس انتشار در شبکه های عصبی، برای به حداقل رساندن تابع هزینه مثل کاری که با گرادیان کاهشی در رگرسیون لجستیک و خطی انجام می‌دادیم، استفاده می‌شود.
هدف ما محاسبه این است:
$$ min_\Theta J(\Theta) $$
یعنی می‌خواهیم تابع هزیه $J$ را با استفاده از یک محموعه بهینه از پارامتر $\Theta$ به حداقل برسانیم (یا به عبارتی دیگر مینیمم کنیم).
در این بخش به معادلاتی که برای محاسبه مشتق جزئی تابع $J(\Theta)$ استفاده می‌کنیم، خواهیم پرداخت:</description>
    </item>
    
    <item>
      <title>پس انتشار قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</link>
      <pubDate>Thu, 08 Oct 2020 12:25:51 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</guid>
      <description>به یاد بیاورید که تابع هزینه برای یک شبکه عصبی به این صورت بود:
$$ \begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*} $$
اگر یک طبقه بندی ساده، و غیر چند کلاسه ($k=1$) را در نظر بگیریم و همچنین منظم سازی را هم نادیده بگیریم، تابع هزینه ما به این صورت محاسبه خواهد شد:</description>
    </item>
    
    <item>
      <title>بازکردن پارامتر ها</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/unrolling-parameters/</link>
      <pubDate>Sat, 10 Oct 2020 13:31:21 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/unrolling-parameters/</guid>
      <description>باز کردن پارامتر ها ما در شبکه های عصبی در حال کار با مجموعه ای از ماتریس ها هستیم: $$ \begin{align*} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align*} $$
برای استفاده از تابع بهینه سازی مثل ()fminunc، می‌خواهیم همه عضو ها را باز کنیم و داخل یک بردار طولانی قرار دهیم:
thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ] deltaVector = [ D1(:); D2(:); D3(:) ]  اگر ابعاد Theta1 $10 \times 11$ و Theta2 $10 \times 11$ و Theta3 $1 \times 11$ باشند، سپس می‌توانیم ماتریس های اصلی مان را از نسخه های باز شده به این شکل برگردانیم:</description>
    </item>
    
    <item>
      <title>بررسی گرادیان</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/gradient-checking/</link>
      <pubDate>Sun, 11 Oct 2020 16:33:27 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/gradient-checking/</guid>
      <description>بررسی گرادیان به شما این اطمینان را می‌دهد که پس انتشار همانطور که در نظر گرفته شده کار می‌کند‌ (مطابق هدف کار می‌کند).
ما می‌توانیم مشتق تابع خود را به این صورت تقریب بزنیم: $$ \frac{\partial}{\partial \Theta} J(\Theta) \approx \frac { J(\Theta + \epsilon) - J(\Theta - \epsilon) } {2 \epsilon} $$
با چند ماتریس تتا، می‌توانیم مشتق را با توجه به $\Theta _j$ تقریب بزنیم:
$$ \frac{\partial}{\partial \Theta} J(\Theta) \approx \frac{ J(\Theta _1, &amp;hellip;, \Theta _j + \epsilon, &amp;hellip;, \Theta _n) - J(\Theta _1, &amp;hellip;, \Theta _j - \epsilon, &amp;hellip;, \Theta _n)} {2 \epsilon} $$</description>
    </item>
    
    <item>
      <title>مقدار دهی اولیه تصادفی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/random-initialization/</link>
      <pubDate>Sun, 11 Oct 2020 17:13:59 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/random-initialization/</guid>
      <description>مقدار دهی اولیه همه وزن های تتا (theta weights) به مقدار $0$ برای شبکه های عصبی کار ساز نیست!
وقتی از پس انتشار استفاده می‌کنیم، همه گره ها به طور مکرر به یک مقدار مشابه به روز می‌شوند. اما در عوض می‌توانیم وزن های ماتریس $\Theta$ خودمان را به روش زیر به صورت تصادفی مقدار دهی کنیم:
از این رو، ما هر $\Theta _{ij} ^{(l)}$ را به صورت عددی تصادفی بین $[ - \epsilon, \epsilon]$ مقدار دهی می‌کنیم، و استفاده از فرمول بالا تضمین می‌کند که این حد مد نظر را به دست می‌آوریم، همین رویه برای همه $\Theta$ ها انجام می‌شود.</description>
    </item>
    
    <item>
      <title>Putting It Together</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/putting-it-together/</link>
      <pubDate>Sun, 11 Oct 2020 19:45:27 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/putting-it-together/</guid>
      <description>ابتدا معماری شبکه خود را انتخاب کنید!
لایه های شبکه عصبی خود را انتخاب کنید، از جمله اینکه چند گره پنهان در هر لایه و در کل چند لایه می‌خواهد داشته باشید.
 تعداد گره های ورودی = ابعاد ویژگی های $x^{(i)}$ تعداد گره های خروجی = تعداد کلاس ها (طبقه بندی ها) تعداد گره های پنهان در هر لایه = معمولا هر چه بیشتر بهتر (افزایش تعداد گره های پنهان باید با هزینه محاسبه آن ها تعادل داشته باشد) پیش فرض ها: ۱ لایه پنهان، اگر بیش از ۱ لایه پنهان دارید پیشنهاد می‌شود که در هر لایه پنهان تعداد گره یکسانی داشته باشید.</description>
    </item>
    
    <item>
      <title>نمونه ای از رانندگی خودکار</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/autonomous-driving/</link>
      <pubDate>Mon, 12 Oct 2020 13:09:44 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/autonomous-driving/</guid>
      <description>پروژه ALVINN ماشین خودران در سال ۱۹۸۹ ALVINN: Autonomous Land Vehicle In a Neural Network
برای مشاهده ویدئو این پروژه در YouTube بر روی تصویر زیر کلیک کنید:

توییت ای جالب در توییتر در این مورد (نیاز به vpn):
GPU? Gez, ALVINN ran on 100 MFLOP CPU, ~10x slower than iWatch; Refrigerator-size &amp;amp; needed 5000 watt generator. @olivercameron pic.twitter.com/QdGpZUzGCs
&amp;mdash; Dean Pomerleau (@deanpomerleau) November 24, 2016  </description>
    </item>
    
    <item>
      <title>فایل های هفته پنجم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/files/</link>
      <pubDate>Mon, 12 Oct 2020 13:33:56 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/files/</guid>
      <description>اسلاید ها  Neural Networks: Learning - pdf  غلط نامه  Errata - pdf  تمرین برنامه نویسی  Programming Exercise 4: Neural Networks Learning - pdf | problem  </description>
    </item>
    
  </channel>
</rss>