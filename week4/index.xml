<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته چهارم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/</link>
    <description>Recent content in  هفته چهارم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Sep 2020 13:28:18 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week4/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>فرضیه غیر خطی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/non-linear-hypotheses/</link>
      <pubDate>Tue, 15 Sep 2020 13:37:07 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/non-linear-hypotheses/</guid>
      <description>انجام رگرسیون لجستیک با مجموعه ای پیچیده از داده ها و ویژگی های زیاد، کار بسیار دشواری است. تصور کنید فرضیه ای با ۳ ویژگی دارید به همراه تمام جملات درجه ۲ آن:
$$ g(\theta_0 + \theta_1 x_1^2 + \theta_2 x_1 x_2 + \theta_3 x_1 x_3 + \theta_4 x_2 ^2 + \theta_5 x_2 x_3 + \theta_6 x_3 ^2 ) $$ می‌بینیم که ۶ ویژگی به ما می‌دهد.
روشی دقیق برای محاسبه تعداد ویژگی ها: $ \frac{(n + r - 1)!</description>
    </item>
    
    <item>
      <title>نورون ها و مغز</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/neurons-and-brain/</link>
      <pubDate>Tue, 15 Sep 2020 19:18:30 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/neurons-and-brain/</guid>
      <description>مبدا و سرچشمه شبکه های عصبی ساخت الگوریتم هایی است که سعی کنند از مغز تقلید کنند. شبکه های عصبی در دهه ۸۰ و ۹۰ میلادی بسیار مورد استفاده قرار می‌گرفته اند، اما در اواخر دهه ۹۰ از محبوبیت آن ها کاسته شد، و اخیرا به دلیل پیشرفت در سخت افزار کامپیوتر ها دوباره احیا شده است.
شواهدی وجود دارد که مغز تنها از یک الگوریتم یادگیری برای انجام تمام عملکرد های محتلف خود استفاده می‌کند.</description>
    </item>
    
    <item>
      <title>ارائه مدل قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-1/</link>
      <pubDate>Thu, 17 Sep 2020 11:13:34 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-1/</guid>
      <description>چگونه یک تابع فرضیه را با استفاده از شبکه های عصبی نشان خواهیم داد؟
شبکه های عصبی به عنوان روشی برای شبیه سازی نورون ها یا شبکه ای از نورون های در مغز ساخته شده اند.
یک نورون در مغز به این شکل است:
که به طور کلی از سه بخش قابل توجه زیر ساخته شده است:
 بدنه سلول بخش ورودی دنریت بخش خروجی اکسون  به طور ساده می‌توانیم بگوییم که:</description>
    </item>
    
    <item>
      <title>ارائه مدل قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-2/</link>
      <pubDate>Sun, 20 Sep 2020 15:54:56 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-2/</guid>
      <description>در این قسمت چگونگی انجام محاسبات را به صورت بهینه تر از طریق پیاده سازی به روش برداری شده بررسی می‌کنیم. و می‌آموزیم که چرا شبکه های عصبی خوب هستند و چطور می‌توانیم از آن ها برای یادگیری چیز های پیچیده و غیر خطی استفاده کنیم.
برای یادآوری عبارات زیر مثالی های شبکه های عصبی بودند:
$$ \begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*} $$</description>
    </item>
    
    <item>
      <title>مثال ها قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-1/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-1/</guid>
      <description>عملگر منطقی AND یک مثال ساده از کاربرد شبکه های عصبی پیش بینی نتیجه عملگر منطقی AND بین دو متغیر $x_1$ و $x_2$ است.
می‌دانیم که جدول درستی عملگر AND به این صورت است:
شکل کلی توابع به صورت زیر است:
$$ \begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}g(z^{(2)})\end{bmatrix} \rightarrow h_\Theta(x)\end{align*} $$
به خاطر داشته باشید که $x_0$ متغیر بایاس ما است، و همواره برابر با مقدار 1 است.
 و ماتریس وزن های خود را به این صورت تنظیم می‌کنیم:</description>
    </item>
    
    <item>
      <title>مثال ها قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-2/</link>
      <pubDate>Wed, 30 Sep 2020 12:06:57 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-2/</guid>
      <description>عملگر XNOR از قسمت قبل به خاطر داریم که ماتریس وزن $\Theta^{(1)}$ برای عملگر های منطقی AND، OR و NOR به این صورت بود: $$ \begin{align*}AND:\newline\Theta^{(1)} &amp;amp;=\begin{bmatrix}-30 &amp;amp; 20 &amp;amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;amp;= \begin{bmatrix}10 &amp;amp; -20 &amp;amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;amp;= \begin{bmatrix}-10 &amp;amp; 20 &amp;amp; 20\end{bmatrix} \newline\end{align*} $$
با ترکیب آن ها می‌توانیم عملگر منطقی XNOR را به دست آوریم: $$ \begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align*} $$</description>
    </item>
    
    <item>
      <title>طبقه بندی چند کلاسه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/multiclass-classification/</link>
      <pubDate>Wed, 30 Sep 2020 16:39:28 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/multiclass-classification/</guid>
      <description>برای طبقه بندی داده ها در چندین کلاس، نیاز داریم که تابع فرضیه ما برداری از مقادیر را برگرداند. مثلا اگر بخواهیم داده هایمان را در یکی از ۴ دسته طبقه بندی کنیم می‌توانیم برای دیدن نحوه انجام این طبقه بندی از مثال زیر استفاده می‌کنیم، این الگوریتم یک تصویر را به عنوان ورودی گرفته و بر اساس آن طبقه بندی را انجام می‌دهد، ۴ دسته ما عبارت اند از:</description>
    </item>
    
    <item>
      <title>فایل های هفته چهارم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/files/</link>
      <pubDate>Wed, 30 Sep 2020 13:04:55 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/files/</guid>
      <description>اسلاید ها  Neural Networks: Representation - pdf  غلط نامه  Errata - pdf  تمرین برنامه نویسی  Programming Exercise 3: Multi-class Classification and Neural Networks - pdf | problem  زیر نویس ها  انگلیسی هننوز ساخته نشده - فارسی  </description>
    </item>
    
  </channel>
</rss>