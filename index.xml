<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>دوره یادگیری ماشین دانشگاه استنفورد به فارسی on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/</link>
    <description>Recent content in دوره یادگیری ماشین دانشگاه استنفورد به فارسی on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 18:40:41 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>فرضیه غیر خطی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/non-linear-hypotheses/</link>
      <pubDate>Tue, 15 Sep 2020 13:37:07 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/non-linear-hypotheses/</guid>
      <description>انجام رگرسیون لجستیک با مجموعه ای پیچیده از داده ها و ویژگی های زیاد، کار بسیار دشواری است. تصور کنید فرضیه ای با ۳ ویژگی دارید به همراه تمام جملات درجه ۲ آن:
$$ g(\theta_0 + \theta_1 x_1^2 + \theta_2 x_1 x_2 + \theta_3 x_1 x_3 + \theta_4 x_2 ^2 + \theta_5 x_2 x_3 + \theta_6 x_3 ^2 ) $$ می‌بینیم که ۶ ویژگی به ما می‌دهد.
روشی دقیق برای محاسبه تعداد ویژگی ها: $ \frac{(n + r - 1)!</description>
    </item>
    
    <item>
      <title>رگرسیون خطی چند متغیره</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/linear-regression-many-variable/</link>
      <pubDate>Wed, 09 Sep 2020 21:11:53 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/linear-regression-many-variable/</guid>
      <description>توی این هفته قراره در مورد رگرسیون خطی با چندین متغیر صحبت کنیم!
مثلا دیتا ای شبیه به این برای خانه ها را فرض کنید:
   نماد      $m$ تعداد کل سطر های جدول داده ها   $n$ تعداد ویژگی ها یا همان متغیر ها   $x^{(i)}$ i امین ردیف از جدول شامل متغیر ها   $x_j^{(i)}$ مقدار موجود در ردیف i ام و ستون متغیر j    بنابراین برای تابع فرضه داریم: $h_\theta = \theta_0 + \theta_1 + \theta_2x + &amp;hellip; + \theta_nx$</description>
    </item>
    
    <item>
      <title>یادگیری ماشین چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</link>
      <pubDate>Sat, 05 Sep 2020 18:40:41 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</guid>
      <description>دو تعریف از یادگیری ماشین ارائه شده است:
 Arthur Samuel: رشته مطالعاتی که به کامپیوتر ها این توانایی را می‌دهد که بدون برنامه نویسی صریح یاد بگیرند.
 توجه: این یک تعریف قدیمی و غیر رسمی است!
 اما تعریفی مدرن تر &amp;hellip;
 Tom Mitchell: به یک برنامه کامپیوتری گفته می‌شود که: برای یادگیری از تجربه E با توجه به برخی از وظایف به عنوان T و اندازه گیری عملکرد با P اگر عملکرد وظیفه T با استفاده از P اندازه گیری شود با استفاده از تجربه E بهبود یابد.</description>
    </item>
    
    <item>
      <title>نورون ها و مغز</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/neurons-and-brain/</link>
      <pubDate>Tue, 15 Sep 2020 19:18:30 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/neurons-and-brain/</guid>
      <description>مبدا و سرچشمه شبکه های عصبی (Neural networks) ساخت الگوریتم هایی است که سعی کنند از مغز تقلید کنند. شبکه های عصبی در دهه ۸۰ و ۹۰ میلادی بسیار مورد استفاده قرار می‌گرفته اند، اما در اواخر دهه ۹۰ از محبوبیت آن ها کاسته شد، و اخیرا به دلیل پشرفت در سخت افزار کامپیوتر ها دوباره احیا شده است.
شواهدی وجود دارد که مغز تنها از یک الگوریتم یادگیری برای انجام تمام عملکرد های محتلف خود استفاده می‌کند.</description>
    </item>
    
    <item>
      <title>طبقه بندی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</link>
      <pubDate>Thu, 10 Sep 2020 12:51:11 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</guid>
      <description>Logistic Regresstion اینجا از مسائل Regression به مسائل Classification می‌رویم، اما با اسم Logistic Regression گیج نشوید! این اسم به دلایل تاریخی نامگذاری شده که در واقع رویکردی برای حل مسائل طبقه بندی است نه رگرسیون!
Binary Classification به جای اینکه خروجی یعنی $y$ مقداری پیوسته در یک محدوده باشد، فقط $0$ یا $1$ است، یعنی : $ y \in \text{{0,1}} $
به طوری که معمولا به $0$، negative class و به $1$ هم positive class می‌گوییم، اما شما آزاد هستید که هر اسم دلخواهی را برای نام‌گذاری آن ها انتخاب کنید!</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی چند متغیره</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/gradient-many-variable/</link>
      <pubDate>Wed, 09 Sep 2020 21:27:26 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/gradient-many-variable/</guid>
      <description>الگوریتم جدید ما برای گرادیان کاهشی با چندین متغیر به این صورت است:
و قسمت $ \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)} ) x_j^{(i)} $ همان مشتق جرئی $\frac {\partial} {\partial\theta_0} J(\theta)$ است.
به طور مثال برای دو متغیره و یا بیشتر خواهیم داشت:
یادآوری: مقدار $x_0$ برابر $1$ است.
 </description>
    </item>
    
    <item>
      <title>یادگیری نظارتی چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:54:05 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</guid>
      <description>تعریف یادگیری نظارتی در یـادگـیری نــظارتـی یک مجموعه داده داریم و از قبل می‌دانیم که خروجی صحیح باید چطور باشد، اصطلاحا داده های لیبل خورده اند! با این ایده که به بین خروجی و ورودی رابطه وجود دارد.
مسائل یادگیری نظارتی یا همان Supervised Learning به دو دسته رگرسیون و طبقه بندی تقسیم می‌شوند.
رگرسیون | Regression در این مسائل سعی می‌کنیم خروجی ای با مقدار پیوسته را پیش بینی کنیم.</description>
    </item>
    
    <item>
      <title>ارائه مدل قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-1/</link>
      <pubDate>Thu, 17 Sep 2020 11:13:34 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-1/</guid>
      <description>چگونه یک تابع فرضیه را با استفاده از شبکه های عصبی نشان خواهیم داد؟
شبکه های عصبی به عنوان روشی برای شبیه سازی نورون ها یا شبکه ای از نورون های در مغز ساخته شده اند.
یک نورون در مغز به این شکل است:
که به طور کلی از سه بخش قابل توجه زیر ساخته شده است:
 بدنه سلول (Cell body) بخش ورودی دنریت (dendrites) بخش خروجی اکسون(axon)  به طور ساده می‌توانیم بگوییم که:</description>
    </item>
    
    <item>
      <title>تابع هزینه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</link>
      <pubDate>Fri, 11 Sep 2020 11:12:45 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</guid>
      <description>ما نمی‌توانیم از همان تابعی هزینه ای که برای رگرسیون خطی استفاده کردیم، برای تابع لجستیک نیز استفاده کنیم، زیرا خروجی تابع لجستیک موج گونه است و باعث ایجاد تعداد زیادی مینیمم محلی می‌شود. به عبارت دیگر یک تابع محدب (convex) نیست.
تابع هزینه ما برای Logistic Regression به این صورت است:
$$ J(\theta) = \frac{1}{m} \sum_{i = 1}^m Cost(h_\theta(x^{(i)}, y^{(i)})) $$
$$ Cost(h_\theta(x), y) = -log(h_\theta(x)) \hspace{1cm} if \hspace{0.3cm} y = 1 $$</description>
    </item>
    
    <item>
      <title>Feature Scaling</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/feature-scaling/</link>
      <pubDate>Wed, 09 Sep 2020 21:48:49 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/feature-scaling/</guid>
      <description>در این قسمت و قسمت بعدی در مورد فوت و فن هایی برای اعمال الگوریتم گرادیـــان کـــاهشی صحبت می‌کنیم.
اگر شما مسئله ای دارید که چندین ویژگی یا متغیر دارد و اگر مطمئن هستید که متغیر ها در مقیاس مشابه ای نسبت به هم هستند، در این حــالت گرادیــــان کـــاهشی با سرعت بیشتری به همگرایی می‌رسد.
فرض کنید مسئله ما دو متغیر به صورت زیر دارد: $$ x_1 = \text {size(0-2000 feet^2) }$$ $$ x_2 = \text {number of bedrooms(1-5) }$$</description>
    </item>
    
    <item>
      <title>یادگیری غیر نظارتی چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:56:04 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</guid>
      <description>تعریف یادگیری غیر نظارتی یادگیری بدون نظارت این امکان را به ما مـی‌دهد کــه بدون داشتن هیچ ایده ای نسبت به خروجی داده ها به حل مشکلات نزدیک شویم. در واقع در اینجا داده های ما هیچ برچسبی نـدارنـد و الگوریتم‌ها به حال خود رها می‌شوند تا سـاختـارهــای موجود در میان داده‌ها را کشف کنند. Unsupervised Learning ها به دو دسته خوشه بندی و غیر خوشه بندی تقسیم می‌شوند.
خوشه بندی | Clustering در این مسـائـــل سـعــی مـی‌کــنیم داده هایی با ویژگی های مشترک را به چـندین گــروه تقـسیم کــنیم، یعنی آن ها را به خوشه ها تخصیص بدهیم.</description>
    </item>
    
    <item>
      <title>ارائه مدل قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-2/</link>
      <pubDate>Sun, 20 Sep 2020 15:54:56 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/model-representation-2/</guid>
      <description>در این قسمت چگونگی انجام محاسبات را به صورت بهینه تر از طریق پیاده سازی به روش vectorized بررسی می‌کنیم. و می‌آموزیم که چرا شبکه های عصبی خوب هستند و چطور می‌توانیم از آن ها برای یادگیری چیز های پیچیده و غیر خطی استفاده کنیم.
برای یادآوری عبارات زیر مثالی های شبکه های عصبی بودند:
$$ \begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*} $$</description>
    </item>
    
    <item>
      <title>ساده شده تابع هزینه و گرادیان کاهشی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</link>
      <pubDate>Fri, 11 Sep 2020 12:53:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</guid>
      <description>Cost Function ما می‌توانیم دو حالت شرطی تابع هزینه خودمان در قسمت قبلی را در یک حالت فشرده شده بنویسیم:
$$ Cost(h_\theta(x), y) = - y \hspace{0.2cm} log(h_\theta(x)) - (1 - y) log(1 - h_\theta(x)) $$
در خاطر داشته باشید وقتی که $y$ برابر $1$ است، قسمت $(1 - y) log(1 - h_\theta(x))$ برار $0$ خواهد شد.
اگر $y$ برابر با $1$ باشد، سپس قسمت $- y \hspace{0.2cm} log(h_\theta(x))$ برابر $0$ خواهد شد و در نتیجه تاثیری ندارد.</description>
    </item>
    
    <item>
      <title>Debugging Gradient</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/debugging-gradient/</link>
      <pubDate>Wed, 09 Sep 2020 22:03:11 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/debugging-gradient/</guid>
      <description>در این قسمت در مورد تکنیک هایی برای اطمینان از درستی کار گرادیان کاهشی صحبت مـی‌کنیم. و در ادامه در مورد نحوه انتخاب مقدار پارامتر آلفا.
همانطور که می‌دانیم کار گرادیان کاهشی پیدا کردن مقدار تتا برای ما است تا تابع هزینه مینیمم شود. می‌خواهیم نمودار تابع $J$ بر حسب دفعات انــــجام گرادیان کاهشی را رسم کنیم و تا متوجه بشویم که گرادیان کاهشی عملکرد درستی دارد یا نه!
به این ترتیب نموداری به این شکل خواهیم داشت: می‌بینیم که احتملا گرادیان کاهشی درست کار مـی‌کند چون بعد از هر بار انجام مقدار $J$ کاهش می‌یابد!</description>
    </item>
    
    <item>
      <title>رگرسیون خطی با یک متغیر</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</link>
      <pubDate>Sun, 06 Sep 2020 13:26:16 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</guid>
      <description>بررسی نماد ها و مفاهیم مثلا در دیتای خانه ها نماد ها به این صورت هستند:    نماد      $m$ تعداد کل ردیف های جدول دیتای یادگیری   $x$ متغیر های ورودی   $y$ متغیر های خروجی یا هدف    برای آدرس دهی در جدول به این شکل عمل می‌کنیم:
$$(x_i, y_i) \Rightarrow x_1= 2104, y_1 = 460$$
اینجا منظور از $i$ اندیس دیتا در جدول است.</description>
    </item>
    
    <item>
      <title>مثال ها قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-1/</link>
      <pubDate>Sun, 27 Sep 2020 16:20:48 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week4/examples-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>بهینه سازی پیشرفته</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</link>
      <pubDate>Fri, 11 Sep 2020 13:28:40 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</guid>
      <description>Conjugate gradient, BFGS و L-BFGS راه های پیچیده تر و سریع تری برای بهینه سازی $\theta$ به جای Gradient descent هستند.
پیشنهاد می‌شود که این الگوریتم های پیچیده را خودتان ننویسید (مگر اینکه در محاسبات عددی متخصص باشید)، و به جای آن از کتابخانه ها استفاده کنید، زیرا قبلا آزمایش شده اند و بسیار بهینه شده اند.
ابتدا لازم است تابعی بسازیم که دو مقدار زیر را با ورودی مقدار $\theta$ برگرداند:</description>
    </item>
    
    <item>
      <title>رگرسیون چند جمله ای</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/polynomial-regression/</link>
      <pubDate>Wed, 09 Sep 2020 22:12:45 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/polynomial-regression/</guid>
      <description>Polynomial Regression تابع فرضیه $h$ می‌تواند خطی نباشد، اگر تناسب خوبی با داده های ما ندارد، می‌توانیم برای تغییر منحنی تابع از توابع چند جمله ای استفاده کنیم تا به تناسب بهتری برای داده ها برسیم.
فرض کنید که تابع فرضیه ما $ h_\theta(x) = \theta_0 + \theta_1 x_1$ باشد بنابراین می‌توانیم ویژگی جدیدی بر پایه ویژگی $x_1$ اضافه کنیم تا به تابعی quadratic یا درجه دوم برسیم:
$$ {\color{Blue} h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2}$$</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</link>
      <pubDate>Sun, 06 Sep 2020 14:08:57 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</guid>
      <description>تعریف تابع هزینه | Cost Function با این تابع می‌توانیم بهترین خط مستقیم را برای داده هایمان به دست آوریم. با انتخاب های متفاوت برای پارامتر های $\theta_1$ و $\theta_0$ تابع های فرضیه متفاوتی به دست می‌آوریم: در رگرسیون خطی مجموعه آموزشی مثل این نمودار داریم و می‌خواهیم مقادیری برای $\theta_0$ و $\theta_1$ به دست آوریم به طوری که خط راستی که رسم می‌کنیم، بهترین تطابق را با داده هایمان داشته باشد.</description>
    </item>
    
    <item>
      <title>طبقه بندی چند کلاسه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/multiclass-classification/</link>
      <pubDate>Sat, 12 Sep 2020 10:56:25 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/multiclass-classification/</guid>
      <description>هنگامی که در classifiction، بیش از دو دسته داشته باشیم به جای $y = \text{ {0,1} }$ تعاریف خود را به $ y = \text { {0,1, &amp;hellip;, n} } $ گسترش می‌دهیم.
از آنجا که ما مسئله خودمان را به n+1 (n+1 به این خاطر که ایندکس از صفر شروع می‌شود) مسئله طبقه بندی باینری تقسیم می‌کنیم، در هر کدام از آن ها ما احتمال عضویت $y$ را در یکی از کلاس هایمان پیش بینی می‌کنیم:</description>
    </item>
    
    <item>
      <title>معادله نرمال</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/normal-equation/</link>
      <pubDate>Thu, 10 Sep 2020 11:44:59 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/normal-equation/</guid>
      <description>Normal Equation الگوریتم گرادیان کاهشی روشی بود برای مینیمم کردن تابع $J$ ، اما روش دومی نیز وجود دارد که بدون داشتن حلقه تکرار این کار را انجام بدهد که معادله نرمال نام دارد.
فرض کنید که تابع هزینه درجه دو ای مثل این داریم: $$ J(\theta) = a\theta^2 + b\theta + c $$ $$ \frac{\partial} {\partial x} J(\theta) \overset{\underset{\mathrm{set}}{}}{=} 0 $$
که برای مینیمم کردن این تابع درجه دو مشتق آن را می‌گیریم و برابر با صفر قرار می‌دهیم، که این به ما اجازه می‌دهد که مقدار $\theta$ را برای مینیمم کردن تابع پیدا کنیم.</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</link>
      <pubDate>Sun, 06 Sep 2020 14:26:42 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</guid>
      <description>تا اینجا به طور خلاصه تمام چیزی که از تابع هزینه می‌دانیم در زیر آمده است:
اما اجازه بدید برای ساده سازی تابع فرضیه را تنها با یک پارامتر به این شکل در نظر بگیریم: $ h_\theta(x) = \theta_1x $ و سه مقدار مختلف $0$، $5.0 $ و $1$ رو حساب کنیم &amp;hellip;
مثلا برای مقدار تتا برابر با $1$ محاسبات زیر را خواهیم داشت:
$$ {\color{Red} J(\theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\theta_1x - y_i)^2 \Rightarrow \frac{1}{2m} (0^2 + 0^2 + 0^2) = 0 } $$ به همین صورت برای دو مقدار دیگر داریم:</description>
    </item>
    
    <item>
      <title>مشکل Overfitting</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/overfitting/</link>
      <pubDate>Sun, 13 Sep 2020 10:33:52 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/overfitting/</guid>
      <description>تصور کنید مسئله پیش بینی $y$ را، به صورتی که $x \in R$ است را داریم.
سمت چپ ترین شکل زیر نتیجه fitting، $y = \theta_0 + \theta_1x$ را بر روی مجموعه داده نشان می‌دهد. می بینیم که داده ها واقعاً روی خط مستقیم قرار ندارند، بنابراین اصطلاحا خوب fit نشده است (تناسب خوبی با داده ها ندارد).
در عوض اگر ویژگی $x^2$ را اضافه کنیم، و $y = \theta_0 + \theta_1x+ \theta_2 x^2$ را fit کنیم، سپس کمی بهتر با داده ها مطابقت پیدا می‌کنیم که در شکل وسطی می‌بینیم.</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</link>
      <pubDate>Sun, 06 Sep 2020 16:31:03 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</guid>
      <description>قسمت قبل دیدیم که با داشتن فقط یک پارامتر برای تابع فرضیه، نمودار تابع هزینه یا همان $J$ به صورت سهمی بود. اگر دو پارامتر داشته باشیم باز هم به صورت سهمی است، اما سه بعدی و بسته به دیتای ما ممکن است به شکل زیر باشد:
اما ما برای نمایش این تابع از شکل سه بعدی استفاده نمی‌کنیم‌، بلکه از نمودار های کانتور استفاده می‌کنیم!
در این نمودار ها هر یک از بیضی ها نشان دهنده مجموعه ای از نقاط است که مقادیر یکسانی در $J$ بر حسب $\theta_0$ و $\theta_1$ های مختلف دارند.</description>
    </item>
    
    <item>
      <title> تابع هزینه در Overfitting</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function-overfitting/</link>
      <pubDate>Sun, 13 Sep 2020 12:19:33 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function-overfitting/</guid>
      <description>اگر تابع فرضیه ما مشکل overfitting دارد، ما می‌توانیم وزن بعضی از بخش های تابع فرضیه را با افزایش هزینه آن ها کاهش دهیم:
تصور کنید که تابع زیر را درجه ۲ (quadratic) تر کنیم: $$ \theta_0 + \theta_1x + \theta_2 x^2 + \theta_3 x^3 +\theta_4 x^4 $$
ما می‌خواهیم تاثیر $\theta_3 x^3$ و $\theta_4 x^4$ را از بین ببریم ، بدون اینکه از شر این ویژگی ها خلاص شویم یا فرم تابع فرضیه خود را تغییر دهیم، ما می‌توانیم تابع هزینه خود را اصلاح کنیم:</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</guid>
      <description>تعریف گرادیان کاهشی | Gradient Descent گرادیان کاهشی را برای مینیمم کردن تابع هزینه $J$ استفاده می‌کنیم. اما این الگوریتم تنها فقط در رگرسیون خطی کاربرد ندارد، بلکه در سایر قسمت های حوزه یادگیری ماشین نیز استفاده می‌شود.
مراحل کار به این شکل است:
با حدس های اولیه برای دو پارامتر $\theta_0$ و $\theta_1$ شروع می‌کنیم، مثلا مقدار هر دو را در ابتدا $0$ تعیین می‌کنیم.
و سپس مقادیر $\theta_0$ و $\theta_1$ را به صورت جزئی تغییر می‌دهیم تا تابع $J$ کاهش یابد، تا زمانی که به مینیمم کلی یا محلی برسیم.</description>
    </item>
    
    <item>
      <title>رگرسیون خطی منظم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-linear-regression/</link>
      <pubDate>Sun, 13 Sep 2020 12:59:40 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-linear-regression/</guid>
      <description>ما می‌توانیم regularization را هم برای رگرسیون خطی و هم برای رگرسیون لجستیک استفاده کنیم. که اینجا ابتدا رگرسیون خطی را بررسی می‌کنیم.
Gradient Descent گرادیان کاهشی را اصلاح می‌کنیم تا $\theta_0$ را از بقیه پارامتر ها جدا کنیم، زیرا نمی‌خواهیم تاثیر $\theta_0$ را کاهش دهیم و از بین ببریم:
$$ \begin{align*} &amp;amp; \text{Repeat}\ \lbrace \newline &amp;amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2&amp;hellip;n\rbrace\newline &amp;amp; \rbrace \end{align*} $$</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</guid>
      <description>در قسمت قبل گرادیان کاهشی را به این صورت معرفی کردیم، در این قسمت می‌خواهیم به توضیح آلفا و عبارت مشتق بپردازیم. اما برای برای درک بهتر می‌خواهیم با یک مثال ساده تر تابعی با یک پارامتر را مینیمم کنیم، یعنی فرض می‌کنیم تابع هزینه $J$ فقط یک پارامتر دارد.
تصور کنید تابع $J$ زیر را با پارامتر تتا یک در این نقطه داریم، و از این نقطه کارمان را شروع می‌کنیم.</description>
    </item>
    
    <item>
      <title>رگرسیون لجستیک منظم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-logistic-regression/</link>
      <pubDate>Tue, 15 Sep 2020 12:48:24 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-logistic-regression/</guid>
      <description>می‌توانیم Logistic Regression را به روشی مشابه رگرسیون خطی منظم سازی کنیم، که در نتیجه می‌توانیم از overfitting پرهیز کنیم.
Cost Function به یاد بیاورید که تابع هزینه ما برای رگرسیون لجستیک به این شکل بود:
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m [y^{(i)} log(h_\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - h_\theta(x^{(i)}) )]
$$
ما می‌توانیم این معادله را با اضافه کردن یک قسمت به انتهای آن منظم کنیم:
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m [y^{(i)} log(h_\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - h_\theta(x^{(i)}) )] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j ^ 2 $$</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</link>
      <pubDate>Wed, 09 Sep 2020 17:40:28 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</guid>
      <description>در این قسمت گرادیان کاهشی را با تابع هزینه ترکیب می‌کنیم و الگوریتم رگرسیون خطی را به دست می‌آوریم. تا اینجای کار به این ها رسیدیم:
اینجا می‌خواهیم از گرادیان کاهشی برای مینیمم کردن تابع هزینه استفاده کنیم! ابتدا تابع $J$ را در الگوریتم گرادیان جاگذاری می‌کنیم و &amp;hellip;
با محاسبه عبارت مشتق جزئی در گرادیان کاهشی برای دو پارامتر $\theta_0$ و $\theta_1$ خواهیم داشت:
$$ \theta_0, j = 0: \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) $$</description>
    </item>
    
  </channel>
</rss>