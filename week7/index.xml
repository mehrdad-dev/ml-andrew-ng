<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته هفتم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/</link>
    <description>Recent content in  هفته هفتم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Oct 2020 14:35:04 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week7/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>بهینه سازی هدفمند</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/optimization-objective/</link>
      <pubDate>Mon, 12 Oct 2020 14:37:24 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/optimization-objective/</guid>
      <description>ماشین بردار پشتیبان (SVM) یکی دیگر از الگوریتم های یادگیری ماشین با نظارت است که گاهی تمیزتر و قدرتمندتر عمل می کند. اگر به خاطر بیاورید، ما در رگریسیون لجستیک از ضوابط زیر استفاده می کردیم:
$$ if\hspace{0.3cm} y=1,\hspace{0.2cm} then \hspace{0.3cm} h_\theta(x)\approx 1 \hspace{0.3cm} and \hspace{0.3cm}\theta\ ^ T x \gg 0 $$
$$ if\hspace{0.3cm} y=0,\hspace{0.2cm} then \hspace{0.3cm} h_\theta(x)\approx 0 \hspace{0.3cm} and \hspace{0.3cm}\theta\ ^ T x \ll 0 $$
تابع هزینه را برای رگریسیون لجستیک (نامنظم) به خاطر بیاورید:</description>
    </item>
    
    <item>
      <title>درک حاشیه اطمینان زیاد</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/large-margin-intuition/</link>
      <pubDate>Thu, 22 Oct 2020 16:30:25 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/large-margin-intuition/</guid>
      <description>یک راه مفید برای فکر کردن راجع به ماشین های بردار پشتیبان این است که آن ها را به عنوان طبقه بندی کننده هایی که حاشیه اطمینان زیادی دارند در نظر بگیرید.
$$ if \hspace{0.3cm} y=1,\hspace{0.1cm} we\hspace{0.2cm} want \hspace{0.3cm} \theta\ ^ T x \ge 1 \hspace{0.3cm}(not\hspace{0.1cm} just\hspace{0.1cm} \ge 0 ) $$
$$ if \hspace{0.3cm} y=0,\hspace{0.1cm} we\hspace{0.2cm} want \hspace{0.3cm} \theta\ ^ T x \le -1 \hspace{0.3cm}(not\hspace{0.1cm} just\hspace{0.1cm} &amp;lt; 0 ) $$</description>
    </item>
    
    <item>
      <title>ریاضیات پشت طبقه بندی با حاشیه اطمینان زیاد (اختیاری)</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/mathematics-behind-large-margin-classification/</link>
      <pubDate>Mon, 26 Oct 2020 23:16:51 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/mathematics-behind-large-margin-classification/</guid>
      <description>ضرب داخلی بردار
فرض کنید دو بردار داریم ، $u$ و $v$:
$$ u= \begin{bmatrix} u_1 \newline u_2 \end{bmatrix} \hspace{0.5cm} v= \begin{bmatrix} v_1 \newline v_2 \end{bmatrix} $$
طول بردار $v$ با $||v||$ نمایش داده می‌شود و خطی را روی نمودار توصیف می‌کند که از مبدا (0,0) شروع شده و تا $(v_1,v_2)$ ادامه دارد.
طبق قضیه فیثاغورث، طول بردار $v$ را می‌توان با فرمول $\sqrt{v_1^2+v_2^2}$ محاسبه کرد.
تصویر بردار $v$ روی بردار $u$ با کشیدن یک زاویه قائم از $u$ به انتهای $v$ و ایجاد یک مثلث قائم الزاویه به دست می‌آید.</description>
    </item>
    
  </channel>
</rss>