<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته هفتم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/</link>
    <description>Recent content in  هفته هفتم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Oct 2020 14:35:04 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week7/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>بهینه سازی هدفمند</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/optimization-objective/</link>
      <pubDate>Mon, 12 Oct 2020 14:37:24 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/optimization-objective/</guid>
      <description>ماشین بردار پشتیبان (SVM) یکی دیگر از الگوریتم های یادگیری ماشین با نظارت است که گاهی تمیزتر و قدرتمندتر عمل می کند. اگر به خاطر بیاورید، ما در رگریسیون لجستیک از ضوابط زیر استفاده می کردیم:
$$ if\hspace{0.3cm} y=1,\hspace{0.2cm} then \hspace{0.3cm} h_\theta(x)\approx 1 \hspace{0.3cm} and \hspace{0.3cm}\theta\ ^ T x \gg 0 $$
$$ if\hspace{0.3cm} y=0,\hspace{0.2cm} then \hspace{0.3cm} h_\theta(x)\approx 0 \hspace{0.3cm} and \hspace{0.3cm}\theta\ ^ T x \ll 0 $$
تابع هزینه را برای رگریسیون لجستیک (نامنظم) به خاطر بیاورید:</description>
    </item>
    
    <item>
      <title>درک حاشیه اطمینان زیاد</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/large-margin-intuition/</link>
      <pubDate>Thu, 22 Oct 2020 16:30:25 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/large-margin-intuition/</guid>
      <description>یک راه مفید برای فکر کردن راجع به ماشین های بردار پشتیبان این است که آن ها را به عنوان طبقه بندی کننده هایی که حاشیه اطمینان زیادی دارند در نظر بگیرید.
$$ if \hspace{0.3cm} y=1,\hspace{0.1cm} we\hspace{0.2cm} want \hspace{0.3cm} \theta\ ^ T x \ge 1 \hspace{0.3cm}(not\hspace{0.1cm} just\hspace{0.1cm} \ge 0 ) $$
$$ if \hspace{0.3cm} y=0,\hspace{0.1cm} we\hspace{0.2cm} want \hspace{0.3cm} \theta\ ^ T x \le -1 \hspace{0.3cm}(not\hspace{0.1cm} just\hspace{0.1cm} &amp;lt; 0 ) $$</description>
    </item>
    
    <item>
      <title>ریاضیات پشت طبقه بندی با حاشیه اطمینان زیاد (اختیاری)</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/mathematics-behind-large-margin-classification/</link>
      <pubDate>Mon, 26 Oct 2020 23:16:51 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/mathematics-behind-large-margin-classification/</guid>
      <description>ضرب داخلی بردار
فرض کنید دو بردار داریم ، $u$ و $v$:
$$ u= \begin{bmatrix} u_1 \newline u_2 \end{bmatrix} \hspace{0.5cm} v= \begin{bmatrix} v_1 \newline v_2 \end{bmatrix} $$
طول بردار $v$ با $||v||$ نمایش داده می‌شود و خطی را روی نمودار توصیف می‌کند که از مبدا (0,0) شروع شده و تا $(v_1,v_2)$ ادامه دارد.
طبق قضیه فیثاغورث، طول بردار $v$ را می‌توان با فرمول $\sqrt{v_1^2+v_2^2}$ محاسبه کرد.
تصویر بردار $v$ روی بردار $u$ با کشیدن یک زاویه قائم از $u$ به انتهای $v$ و ایجاد یک مثلث قائم الزاویه به دست می‌آید.</description>
    </item>
    
    <item>
      <title>کرنل ها قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/kernels1/</link>
      <pubDate>Tue, 03 Nov 2020 11:24:39 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/kernels1/</guid>
      <description>کرنل ها به ما این اجازه را می دهند تا با استفاده از ماشین بردار پشتیبان، طبقه بندی کننده های پیچیده و غیرخطی بسازیم.
با توجه به x ، ویژگی جدید را بسته به نزدیکی به نقاط عطف $l^{(3)}, l^{(2)}, l^{(1)}$ محاسبه کنید.
برای انجام این کار ، ما &amp;ldquo;شباهت&amp;rdquo; x و برخی از نقاط عطف $l^{(i)}$ را پیدا می کنیم.
$$ f_i=similarity(x,l^{(i)})=exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) $$
به این تابع &amp;ldquo;شباهت&amp;rdquo; کرنل گاوسی گفته می شود.</description>
    </item>
    
    <item>
      <title>کرنل ها قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/kernels2/</link>
      <pubDate>Tue, 17 Nov 2020 16:40:46 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/kernels2/</guid>
      <description>یکی از راه های بدست آوردن نقطه های عطف این است که آن ها را دقیقا در مکان همه نمونه های آموزشی قرار دهیم. این کار به ما m نقطه عطف می دهد، با یک نقطه عطف به ازای هر نمونه آموزشی.
با توجه به x:
$$ f_1=similarity(x,l^{(1)}),\hspace{0.1cm} f_2=similarity(x,l^{(2)}),\hspace{0.1cm}f_3=similarity(x,l^{(3)}),&amp;hellip; $$
این به ما یک «بردار ویژگی» می دهد، $f_{(i)}$ همه ویژگی های ما برای نمونه $x_{(i)}$. هم چنین می توانیم $f_0=1$ قرار دهیم تا مطابق با $\Theta_0$ شود.</description>
    </item>
    
    <item>
      <title>فایل های هفته هفتم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/files/</link>
      <pubDate>Sun, 29 Nov 2020 17:11:38 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/files/</guid>
      <description>اسلاید ها  Support vector machines - pdf  غلط نامه  Errata - pdf  تمرین برنامه نویسی  Programming Exercise 6: Support Vector Machines - pdf | problem  </description>
    </item>
    
    <item>
      <title>منابع بیشتر</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week7/additional-refrences/</link>
      <pubDate>Sat, 21 Nov 2020 17:19:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week7/additional-refrences/</guid>
      <description>&amp;ldquo;An Idiot&amp;rsquo;s Guide to Support Vector Machines&amp;rdquo;: http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</description>
    </item>
    
  </channel>
</rss>