<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته سوم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/</link>
    <description>Recent content in  هفته سوم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 18:30:44 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week3/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>طبقه بندی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</link>
      <pubDate>Thu, 10 Sep 2020 12:51:11 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</guid>
      <description>رگرسیون لجستیک اینجا از مسائل رگرسیون به مسائل طبقه بندی می‌رویم، اما با اسم رگرسیون لجستیک گیج نشوید! این اسم به دلایل تاریخی نامگذاری شده که در واقع رویکردی برای حل مسائل طبقه بندی است نه رگرسیون!
طبقه دودویی به جای اینکه خروجی یعنی $y$ مقداری پیوسته در یک محدوده باشد، فقط $0$ یا $1$ است، یعنی : $ y \in \text{{0,1}} $
به طوری که معمولا به $0$، negative class و به $1$ هم positive class می‌گوییم، اما شما آزاد هستید که هر اسم دلخواهی را برای نام‌گذاری آن ها انتخاب کنید!</description>
    </item>
    
    <item>
      <title>تابع هزینه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</link>
      <pubDate>Fri, 11 Sep 2020 11:12:45 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</guid>
      <description>ما نمی‌توانیم از همان تابعی هزینه ای که برای رگرسیون خطی استفاده کردیم، برای تابع لجستیک نیز استفاده کنیم، زیرا خروجی تابع لجستیک موج گونه است و باعث ایجاد تعداد زیادی مینیمم محلی می‌شود. به عبارت دیگر یک تابع محدب نیست.
تابع هزینه ما برای رگرسیون لجستیک به این صورت است:
$$ J(\theta) = \frac{1}{m} \sum_{i = 1}^m Cost(h_\theta(x^{(i)}, y^{(i)})) $$
$$ Cost(h_\theta(x), y) = -log(h_\theta(x)) \hspace{1cm} if \hspace{0.3cm} y = 1 $$</description>
    </item>
    
    <item>
      <title>ساده شده تابع هزینه و گرادیان کاهشی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</link>
      <pubDate>Fri, 11 Sep 2020 12:53:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</guid>
      <description>تابع هزینه ما می‌توانیم دو حالت شرطی تابع هزینه خودمان در قسمت قبلی را در یک حالت فشرده شده بنویسیم:
$$ Cost(h_\theta(x), y) = - y \hspace{0.2cm} log(h_\theta(x)) - (1 - y) log(1 - h_\theta(x)) $$
در خاطر داشته باشید وقتی که $y$ برابر $1$ است، قسمت $(1 - y) log(1 - h_\theta(x))$ برار $0$ خواهد شد.
اگر $y$ برابر با $1$ باشد، سپس قسمت $- y \hspace{0.2cm} log(h_\theta(x))$ برابر $0$ خواهد شد و در نتیجه تاثیری ندارد.</description>
    </item>
    
    <item>
      <title>بهینه سازی پیشرفته</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</link>
      <pubDate>Fri, 11 Sep 2020 13:28:40 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</guid>
      <description>Conjugate gradient, BFGS و L-BFGS راه های پیچیده تر و سریع تری برای بهینه سازی $\theta$ به جای گرادیان کاهشی هستند.
پیشنهاد می‌شود که این الگوریتم های پیچیده را خودتان ننویسید (مگر اینکه در محاسبات عددی متخصص باشید)، و به جای آن از کتابخانه ها استفاده کنید، زیرا قبلا آزمایش شده اند و بسیار بهینه شده اند.
ابتدا لازم است تابعی بسازیم که دو مقدار زیر را با ورودی مقدار $\theta$ برگرداند:</description>
    </item>
    
    <item>
      <title>طبقه بندی چند کلاسه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/multiclass-classification/</link>
      <pubDate>Sat, 12 Sep 2020 10:56:25 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/multiclass-classification/</guid>
      <description>هنگامی که در طبقه بندی، بیش از دو دسته داشته باشیم به جای $y = \text{ {0,1} }$ تعاریف خود را به $ y = \text { {0,1, &amp;hellip;, n} } $ گسترش می‌دهیم.
از آنجا که ما مسئله خودمان را به n+1 (n+1 به این خاطر که ایندکس از صفر شروع می‌شود) مسئله طبقه بندی باینری تقسیم می‌کنیم، در هر کدام از آن ها ما احتمال عضویت $y$ را در یکی از کلاس هایمان پیش بینی می‌کنیم:</description>
    </item>
    
    <item>
      <title>مشکل Overfitting</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/overfitting/</link>
      <pubDate>Sun, 13 Sep 2020 10:33:52 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/overfitting/</guid>
      <description>تصور کنید مسئله پیش بینی $y$ را، به صورتی که $x \in R$ است را داریم.
سمت چپ ترین شکل زیر نتیجه fitting، $y = \theta_0 + \theta_1x$ را بر روی مجموعه داده نشان می‌دهد. می بینیم که داده ها واقعاً روی خط مستقیم قرار ندارند، بنابراین اصطلاحا خوب fit نشده است (تناسب خوبی با داده ها ندارد).
در عوض اگر ویژگی $x^2$ را اضافه کنیم، و $y = \theta_0 + \theta_1x+ \theta_2 x^2$ را fit کنیم، سپس کمی بهتر با داده ها مطابقت پیدا می‌کنیم که در شکل وسطی می‌بینیم.</description>
    </item>
    
    <item>
      <title> تابع هزینه در Overfitting</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function-overfitting/</link>
      <pubDate>Sun, 13 Sep 2020 12:19:33 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function-overfitting/</guid>
      <description>اگر تابع فرضیه ما مشکل overfitting دارد، ما می‌توانیم وزن بعضی از بخش های تابع فرضیه را با افزایش هزینه آن ها کاهش دهیم:
تصور کنید که تابع زیر را درجه دو تر کنیم: $$ \theta_0 + \theta_1x + \theta_2 x^2 + \theta_3 x^3 +\theta_4 x^4 $$
ما می‌خواهیم تاثیر $\theta_3 x^3$ و $\theta_4 x^4$ را از بین ببریم ، بدون اینکه از شر این ویژگی ها خلاص شویم یا فرم تابع فرضیه خود را تغییر دهیم، ما می‌توانیم تابع هزینه خود را اصلاح کنیم:</description>
    </item>
    
    <item>
      <title>رگرسیون خطی منظم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-linear-regression/</link>
      <pubDate>Sun, 13 Sep 2020 12:59:40 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-linear-regression/</guid>
      <description>ما می‌توانیم منظم سازی را هم برای رگرسیون خطی و هم برای رگرسیون لجستیک استفاده کنیم. که اینجا ابتدا رگرسیون خطی را بررسی می‌کنیم.
گرادیان کاهشی گرادیان کاهشی را اصلاح می‌کنیم تا $\theta_0$ را از بقیه پارامتر ها جدا کنیم، زیرا نمی‌خواهیم تاثیر $\theta_0$ را کاهش دهیم و از بین ببریم:
$$ \begin{align*} &amp;amp; \text{Repeat}\ \lbrace \newline &amp;amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp;amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2&amp;hellip;n\rbrace\newline &amp;amp; \rbrace \end{align*} $$</description>
    </item>
    
    <item>
      <title>رگرسیون لجستیک منظم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-logistic-regression/</link>
      <pubDate>Tue, 15 Sep 2020 12:48:24 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/regularized-logistic-regression/</guid>
      <description>می‌توانیم رگرسیون لجستیک را به روشی مشابه رگرسیون خطی منظم سازی کنیم، که در نتیجه می‌توانیم از overfitting پرهیز کنیم.
تابع هزینه به یاد بیاورید که تابع هزینه ما برای رگرسیون لجستیک به این شکل بود:
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m [y^{(i)} log(h_\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - h_\theta(x^{(i)}) )]
$$
ما می‌توانیم این معادله را با اضافه کردن یک قسمت به انتهای آن منظم کنیم:
$$ J(\theta) = - \frac{1}{m} \sum_{i=1}^m [y^{(i)} log(h_\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - h_\theta(x^{(i)}) )] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j ^ 2 $$</description>
    </item>
    
    <item>
      <title>فایل های هفته سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/files/</link>
      <pubDate>Wed, 30 Sep 2020 13:04:49 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/files/</guid>
      <description>اسلاید ها  Logistic regression - pdf Regularization - pdf  غلط نامه  Errata - pdf  تمرین برنامه نویسی  Programming Exercise 2: Logistic Regression - pdf | problem  </description>
    </item>
    
  </channel>
</rss>