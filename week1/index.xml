<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>   هفته اول  on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/</link>
    <description>Recent content in    هفته اول  on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 18:30:44 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week1/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>یادگیری ماشین چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</link>
      <pubDate>Sat, 05 Sep 2020 18:40:41 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</guid>
      <description>دو تعریف از یادگیری ماشین ارائه شده است:
 Arthur Samuel: رشته مطالعاتی که به کامپیوتر ها این توانایی را می‌دهد که بدون برنامه نویسی صریح یاد بگیرند.
 توجه: این یک تعریف قدیمی و غیر رسمی است!
 اما تعریفی مدرن تر &amp;hellip;
 Tom Mitchell: به یک برنامه کامپیوتری گفته می‌شود که: برای یادگیری از تجربه E با توجه به برخی از وظایف به عنوان T و اندازه گیری عملکرد با P اگر عملکرد وظیفه T با استفاده از P اندازه گیری شود با استفاده از تجربه E بهبود یابد.</description>
    </item>
    
    <item>
      <title>یادگیری با نظارت چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:54:05 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</guid>
      <description>تعریف یادگیری با نظارت در یـادگـیری با نظارت یک مجموعه داده داریم و از قبل می‌دانیم که خروجی صحیح باید چطور باشد، اصطلاحا داده های لیبل خورده اند! با این ایده که به بین خروجی و ورودی رابطه وجود دارد.
مسائل یادگیری با نظارت به دو دسته رگرسیون و طبقه بندی تقسیم می‌شوند.
رگرسیون | Regression در این مسائل سعی می‌کنیم خروجی ای با مقدار پیوسته را پیش بینی کنیم.</description>
    </item>
    
    <item>
      <title>یادگیری بدون نظارت چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:56:04 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</guid>
      <description>تعریف یادگیری بدون نظارت یادگیری بدون نظارت این امکان را به ما مـی‌دهد کــه بدون داشتن هیچ ایده ای نسبت به خروجی داده ها به حل مشکلات نزدیک شویم. در واقع در اینجا داده های ما هیچ برچسبی نـدارنـد و الگوریتم‌ها به حال خود رها می‌شوند تا سـاختـارهــای موجود در میان داده‌ها را کشف کنند. Unsupervised Learning ها به دو دسته خوشه بندی و غیر خوشه بندی تقسیم می‌شوند.
خوشه بندی | Clustering در این مسـائـــل سـعــی مـی‌کــنیم داده هایی با ویژگی های مشترک را به چـندین گــروه تقـسیم کــنیم، یعنی آن ها را به خوشه ها تخصیص بدهیم.</description>
    </item>
    
    <item>
      <title>رگرسیون خطی با یک متغیر</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</link>
      <pubDate>Sun, 06 Sep 2020 13:26:16 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</guid>
      <description>بررسی نماد ها و مفاهیم مثلا در داده ی خانه ها نماد ها به این صورت هستند:    نماد      $m$ تعداد کل ردیف های جدول داده آموزش   $x$ متغیر های ورودی   $y$ متغیر های خروجی یا هدف    برای آدرس دهی در جدول به این شکل عمل می‌کنیم:
$$(x_i, y_i) \Rightarrow x_1= 2104, y_1 = 460$$
اینجا منظور از $i$ اندیس داده در جدول است.</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</link>
      <pubDate>Sun, 06 Sep 2020 14:08:57 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</guid>
      <description>تابع هزینه | Cost Function با این تابع می‌توانیم بهترین خط مستقیم را برای داده هایمان به دست آوریم. با انتخاب های متفاوت برای پارامتر های $\theta_1$ و $\theta_0$ تابع های فرضیه متفاوتی به دست می‌آوریم: در رگرسیون خطی مجموعه آموزشی مثل این نمودار داریم و می‌خواهیم مقادیری برای $\theta_0$ و $\theta_1$ به دست آوریم به طوری که خط راستی که رسم می‌کنیم، بهترین تطابق را با داده هایمان داشته باشد.</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</link>
      <pubDate>Sun, 06 Sep 2020 14:26:42 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</guid>
      <description>تا اینجا به طور خلاصه تمام چیزی که از تابع هزینه می‌دانیم در زیر آمده است:
اما اجازه بدید برای ساده سازی تابع فرضیه را تنها با یک پارامتر به این شکل در نظر بگیریم: $ h_\theta(x) = \theta_1x $ و سه مقدار مختلف $0$، $5.0 $ و $1$ رو حساب کنیم &amp;hellip;
مثلا برای مقدار تتا برابر با $1$ محاسبات زیر را خواهیم داشت:
$$ {\color{Red} J(\theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\theta_1x - y_i)^2 \Rightarrow \frac{1}{2m} (0^2 + 0^2 + 0^2) = 0 } $$ به همین صورت برای دو مقدار دیگر داریم:</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</link>
      <pubDate>Sun, 06 Sep 2020 16:31:03 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</guid>
      <description>قسمت قبل دیدیم که با داشتن فقط یک پارامتر برای تابع فرضیه، نمودار تابع هزینه یا همان $J$ به صورت سهمی بود. اگر دو پارامتر داشته باشیم باز هم به صورت سهمی است، اما سه بعدی و بسته به داده ما ممکن است به شکل زیر باشد:
اما ما برای نمایش این تابع از شکل سه بعدی استفاده نمی‌کنیم‌، بلکه از نمودار های کانتور استفاده می‌کنیم!
در این نمودار ها هر یک از بیضی ها نشان دهنده مجموعه ای از نقاط است که مقادیر یکسانی در $J$ بر حسب $\theta_0$ و $\theta_1$ های مختلف دارند.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</guid>
      <description>گرادیان کاهشی | Gradient Descent گرادیان کاهشی را برای مینیمم کردن تابع هزینه $J$ استفاده می‌کنیم. اما این الگوریتم تنها فقط در رگرسیون خطی کاربرد ندارد، بلکه در سایر قسمت های حوزه یادگیری ماشین نیز استفاده می‌شود.
مراحل کار به این شکل است:
با حدس های اولیه برای دو پارامتر $\theta_0$ و $\theta_1$ شروع می‌کنیم، مثلا مقدار هر دو را در ابتدا $0$ تعیین می‌کنیم.
و سپس مقادیر $\theta_0$ و $\theta_1$ را به صورت جزئی تغییر می‌دهیم تا تابع $J$ کاهش یابد، تا زمانی که به مینیمم کلی یا محلی برسیم.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</guid>
      <description>در قسمت قبل گرادیان کاهشی را به این صورت معرفی کردیم، در این قسمت می‌خواهیم به توضیح آلفا و عبارت مشتق بپردازیم. اما برای برای درک بهتر می‌خواهیم با یک مثال ساده تر تابعی با یک پارامتر را مینیمم کنیم، یعنی فرض می‌کنیم تابع هزینه $J$ فقط یک پارامتر دارد.
تصور کنید تابع $J$ زیر را با پارامتر $\theta_1$ در این نقطه داریم، و از این نقطه کارمان را شروع می‌کنیم.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</link>
      <pubDate>Wed, 09 Sep 2020 17:40:28 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</guid>
      <description>در این قسمت گرادیان کاهشی را با تابع هزینه ترکیب می‌کنیم و الگوریتم رگرسیون خطی را به دست می‌آوریم. تا اینجای کار به این ها رسیدیم:
اینجا می‌خواهیم از گرادیان کاهشی برای مینیمم کردن تابع هزینه استفاده کنیم! ابتدا تابع $J$ را در الگوریتم گرادیان جاگذاری می‌کنیم و &amp;hellip;
با محاسبه عبارت مشتق جزئی در گرادیان کاهشی برای دو پارامتر $\theta_0$ و $\theta_1$ خواهیم داشت:
$$ \theta_0, j = 0: \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) $$</description>
    </item>
    
    <item>
      <title>فایل های هفته اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/files/</link>
      <pubDate>Wed, 30 Sep 2020 12:46:19 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/files/</guid>
      <description>اسلاید ها  Welcome - pdf Linear regression with one variable - pdf Linear Algebra review (Optional) - pdf  غلط نامه  Errata - pdf  </description>
    </item>
    
  </channel>
</rss>